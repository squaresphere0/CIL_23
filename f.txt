PixelSwinT(
  (swin): SwinTransformerV2(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    )
    (layers): Sequential(
      (0): SwinTransformerV2Stage(
        (downsample): Identity()
        (blocks): ModuleList(
          (0): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=6, bias=False)
              )
              (qkv): Linear(in_features=192, out_features=576, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (drop_path1): Identity()
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (drop_path2): Identity()
          )
          (1): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=6, bias=False)
              )
              (qkv): Linear(in_features=192, out_features=576, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.004)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.004)
          )
        )
      )
      (1): SwinTransformerV2Stage(
        (downsample): PatchMerging(
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (blocks): ModuleList(
          (0): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=12, bias=False)
              )
              (qkv): Linear(in_features=384, out_features=1152, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.009)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.009)
          )
          (1): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=12, bias=False)
              )
              (qkv): Linear(in_features=384, out_features=1152, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.013)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.013)
          )
        )
      )
      (2): SwinTransformerV2Stage(
        (downsample): PatchMerging(
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (blocks): ModuleList(
          (0): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.017)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.017)
          )
          (1): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.022)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.022)
          )
          (2): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.026)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.026)
          )
          (3): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.030)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.030)
          )
          (4): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.035)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.035)
          )
          (5): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.039)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.039)
          )
          (6): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.043)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.043)
          )
          (7): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.048)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.048)
          )
          (8): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.052)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.052)
          )
          (9): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.057)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.057)
          )
          (10): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.061)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.061)
          )
          (11): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.065)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.065)
          )
          (12): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.070)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.070)
          )
          (13): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.074)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.074)
          )
          (14): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.078)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.078)
          )
          (15): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.083)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.083)
          )
          (16): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.087)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.087)
          )
          (17): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=24, bias=False)
              )
              (qkv): Linear(in_features=768, out_features=2304, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.091)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.091)
          )
        )
      )
      (3): SwinTransformerV2Stage(
        (downsample): PatchMerging(
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
        (blocks): ModuleList(
          (0): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=48, bias=False)
              )
              (qkv): Linear(in_features=1536, out_features=4608, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.096)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.096)
          )
          (1): SwinTransformerV2Block(
            (attn): WindowAttention(
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=48, bias=False)
              )
              (qkv): Linear(in_features=1536, out_features=4608, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1536, out_features=1536, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (drop_path1): DropPath(drop_prob=0.100)
            (mlp): Mlp(
              (fc1): Linear(in_features=1536, out_features=6144, bias=True)
              (act): GELU()
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=6144, out_features=1536, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            (drop_path2): DropPath(drop_prob=0.100)
          )
        )
      )
    )
    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
    (head): Identity()
  )
  (resize): Resize(size=(384, 384), interpolation=bilinear, max_size=None, antialias=None)
  (reduce_channels): Conv2d(1536, 1, kernel_size=(1, 1), stride=(1, 1))
  (up0): Sequential(
    (0): ConvTranspose2d(1536, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (up1): Sequential(
    (0): ConvTranspose2d(1536, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (up2): Sequential(
    (0): ConvTranspose2d(1152, 576, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (not_up3): Sequential(
    (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))
    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (up4): Sequential(
    (0): ConvTranspose2d(768, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (up5): Sequential(
    (0): ConvTranspose2d(384, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (upsample): Upsample(size=(400, 400), mode=bicubic)
  (batchnorm): Sequential(
    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)